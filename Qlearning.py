class QLearningAgent:
    def __init__(self, maze, alpha=0.1, gamma=0.9, epsilon=0.1):
        self.maze = maze
        self.height, self.width = maze.shape
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.epsilon = epsilon  # Exploration rate

        # Kh·ªüi t·∫°o Q-table: M·ªói √¥ s·∫Ω c√≥ 4 gi√° tr·ªã ·ª©ng v·ªõi 4 h∆∞·ªõng ƒëi (Up, Down, Left, Right)
        self.q_table = np.zeros((self.height, self.width, 4))

    def get_valid_actions(self, x, y):
        """L·∫•y c√°c h√†nh ƒë·ªông h·ª£p l·ªá (kh√¥ng ƒëi v√†o t∆∞·ªùng)"""
        actions = []
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right

        for i, (dx, dy) in enumerate(moves):
            nx, ny = x + dx, y + dy
            if 0 <= nx < self.height and 0 <= ny < self.width and self.maze[nx, ny] == 0:
                actions.append(i)

        return actions

    def choose_action(self, x, y):
        """Ch·ªçn h√†nh ƒë·ªông theo Q-Learning (Exploration vs Exploitation)"""
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(self.get_valid_actions(x, y))  # Ch·ªçn ng·∫´u nhi√™n
        else:
            return np.argmax(self.q_table[x, y])  # Ch·ªçn h√†nh ƒë·ªông c√≥ gi√° tr·ªã Q cao nh·∫•t

    def update_q_table(self, x, y, action, reward, next_x, next_y):
        """C·∫≠p nh·∫≠t gi√° tr·ªã Q-table d·ª±a tr√™n tr·∫£i nghi·ªám"""
        best_next_q = np.max(self.q_table[next_x, next_y])  # Gi√° tr·ªã Q t·ªët nh·∫•t c·ªßa tr·∫°ng th√°i ti·∫øp theo
        current_q = self.q_table[x, y, action]

        # C√¥ng th·ª©c c·∫≠p nh·∫≠t Q-Learning
        self.q_table[x, y, action] = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * best_next_q)

    def train(self, start, goal, episodes=1000, max_steps=100):
        """Hu·∫•n luy·ªán AI t√¨m ƒë∆∞·ªùng tho√°t kh·ªèi m√™ cung"""
        for episode in range(episodes):
            x, y = start
            for _ in range(max_steps):
                action = self.choose_action(x, y)

                # Th·ª±c hi·ªán di chuy·ªÉn
                moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]
                dx, dy = moves[action]
                nx, ny = x + dx, y + dy

                # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng
                if (nx, ny) == goal:
                    reward = 100  # Th∆∞·ªüng l·ªõn n·∫øu AI t√¨m th·∫•y l·ªëi ra
                    self.update_q_table(x, y, action, reward, nx, ny)
                    break
                elif self.maze[nx, ny] == 1:
                    reward = -10  # Ph·∫°t n·∫øu ƒë√¢m v√†o t∆∞·ªùng
                else:
                    reward = -1  # Ph·∫°t nh·∫π ƒë·ªÉ khuy·∫øn kh√≠ch ƒëi nhanh

                # C·∫≠p nh·∫≠t Q-table
                self.update_q_table(x, y, action, reward, nx, ny)
                x, y = nx, ny

            if (episode + 1) % 100 == 0:
                print(f"Episode {episode + 1}/{episodes} - AI ƒëang h·ªçc...")

    def solve(self, start, goal):
        """Ch·∫°y AI sau khi h·ªçc xong"""
        x, y = start
        path = [(x, y)]

        for _ in range(100):  # Gi·ªõi h·∫°n s·ªë b∆∞·ªõc
            action = np.argmax(self.q_table[x, y])  # Ch·ªçn h∆∞·ªõng t·ªët nh·∫•t theo Q-table
            dx, dy = [(-1, 0), (1, 0), (0, -1), (0, 1)][action]
            nx, ny = x + dx, y + dy

            if (nx, ny) == goal:
                path.append((nx, ny))
                return path  # T√¨m th·∫•y l·ªëi ra

            if self.maze[nx, ny] == 1:  # N·∫øu AI ƒëi v√†o t∆∞·ªùng, d·ª´ng l·∫°i
                break

            x, y = nx, ny
            path.append((x, y))

        return None  # Kh√¥ng t√¨m th·∫•y l·ªëi ra
    

import numpy as np
import random


# T·∫°o m√™ cung
maze, start = create_maze_dfs(WIDTH, HEIGHT)
goal = (WIDTH - 2, HEIGHT - 2)

# Hu·∫•n luy·ªán AI
agent = QLearningAgent(maze)
print("üîÑ ƒêang hu·∫•n luy·ªán AI...")
agent.train(start, goal)

# Ch·∫°y AI sau khi hu·∫•n luy·ªán
print("‚úÖ AI ƒëang gi·∫£i m√™ cung...")
path = agent.solve(start, goal)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
if path:
    for x, y in path:
        maze[x, y] = 3  # ƒê√°nh d·∫•u ƒë∆∞·ªùng ƒëi
    display_maze(maze, start, goal)
    print(f"üéâ AI t√¨m th·∫•y l·ªëi tho√°t trong {len(path)} b∆∞·ªõc!")
else:
    print("‚ùå AI v·∫´n ch∆∞a t√¨m ƒë∆∞·ª£c l·ªëi ra.")
